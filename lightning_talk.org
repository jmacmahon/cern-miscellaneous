* Things it would be hard to do with unix tools but easy with ELK
** Most common status codes
pv -Webrapt -l apache.log | sed -r 's/^.* "[A-Z]+ .* HTTP\/1\.." (...).*$/\1/' | sort | uniq -c | sort -n

** Most common URLs
pv -Webrapt -l apache.log | sed -r 's/^.* "[A-Z]+ (.*) HTTP\/1\..".*$/\1/' | sort | uniq -c | sort -n | tail -n 50

** Number and %age of requests from FR/CH?
head -n 50000 apache.log | pv -Webrapt -l -s 50000 | sed -r 's/^(([0-9]+\.){3}[0-9]+) .*$/\1/' | xargs -n 1 geoiplookup | sed -r 's/^.*: (..), .*$/\1/' | sort | uniq -c | sort -n

** Number of 403s from Googlebot
grep -i 'googlebot' apache.log | sed -r 's/^.* "[A-Z]+ .* HTTP\/1\.." (...).*$/\1/' | sort | uniq -c | sort -n

* Script
** Act 1
N Hey Joe, here's a 5GB Apache log file. can you tell me how many times
  this URL was accessed in February?

J [15 minutes later] 90

N and how many of these times were from the US?

J From the US?  Uhh... exactly 21

N And what about excluding redirects?

J [despair]

N [To Joe] Don't you wish there was an easier way to do this?

N [To audience] Are you tired of using grep and sed to process your logs?  Are
  you tired of loading 5GB files in vi?

J [Interrupting] Umm, actually Emacs is a lot faster!

N Whatever.

N Are your logs working for you, or are you working for your logs?

** Act 2
J [Opens Kibana]

N Today's the day to take the power back!
  So, Joe, how about those numbers then?

J [Highlighting figures] So the number of hits for that URL in February was
  about 90, from the US we had about 21, and then excluding redirects we
  got 17.  Look, here's a graph!

N Wow!  That looks good.
  [To self] But hold on, wait a second, how did we get from those logs to these
  graphs?

N [To audience] Let me introduce you to our expert, Mr. MacMahon

N [To J] So Joseph, why don't you walk us through it?

** Act 3
J [With presentation] Well Nikos, at the heart of all this is
  Elasticsearch -- a high performance, distributed search
  engine, optimised for storing log data.  We take the lines coming
  into our Apache logs using another piece of software called Logstash,
  store them in Elasticsearch, and then use this web interface, called
  Kibana, to visualise and analyse our data.

[Sline: ES, Logstash, Kibana logos, in fragments, maybe some lines between
them to show progression of logs?]

N It must have taken some work to set all that up!

J Well actually, you won't believe how easy it is!  I'll tell you how we did it
  here for CDS.  We have a ten-machine Elasticsearch cluster, with 1TB of
  storage on each node --

[Slide: 10 machines, next fragment 1TB on each]

N [interrupting] That's 10TB!

J Yes, according to our calculations this will last us for 50 years of log
  data, and that's with replication!  We use Openstack and Puppet to painlessly
  deploy and configure all the hardware and software that we need.

[Slide: Openstack + Puppet logos (1 slide, no fragments)]

N How do you manage the cluster?

J [Open Kopf in browser] One of the many plugins to really enrich the
  functionality of Elasticsearch is called Kopf.  Let me just walk you
  through this interface.  Going down the left hand side we have a row
  for each node, showing some info about resource usage, and then
  across the top we have all of our Elasticsearch indices (more?).  As
  you can see, each index is split into 10 shards, each with a single
  replica, and get this: the cluster manages this whole setup for you.
  No manual balancing, no master/slave configuration, you just sit
  back, relax and let Elasticsearch take care of your data for you!

N Joseph, I saw you had this really nice map just now.

J That's another thing we have Logstash to thank for!  As our Apache
  logs come in, Logstash tags each record with a country code, based on
  a GeoIP lookup.  That data goes straight into the cluster using JSON
  over HTTP, and Kibana picks it right back out again when we visualise
  it.  There's lots of other cool stuff you can do with Logstash, for
  example tagging every record that was generated by a bot.

[Slide: map]

N Did you say you transfer the data over plain HTTP?

J Well Nikos, I'm so glad you asked me that.  We found that by using nginx, a
  lightweight reverse proxy server, we can tunnel our JSON over HTTPS instead --
  and not only that!  Nginx will also take care of balancing the indexing queries
  across all the nodes in the cluster all by itself!  Piece of cake.

[Slide: cluster diagram, now with HTTPS/JSON tunnel]

N Wow.  Let's see what real users have to say.

[testimonials]

N Sensational!  Joseph, this all sounds fantastic, but it's gotta come with a
  pretty hefty price tag, right?

[Slide: $$$ or blank]

J This is my favourite part!  All of the software we've shown you today is
  totally gratis and open source!  Bet you didn't see that one coming!

[Slide: All logos: ES, Logstash, Kibana, Puppet, Openstack, Nginx,
JSON, KOPF?]

N Don't wait any longer!  Put your logs to work today!

* Notes
Next 3 questions should be

- kopf/plugins/indices -- management
- logs, bots, geoip, json, http
- nginx, security, balancing
- lifecycle/workflow

Next segments:

- Testimonials
** Closing
- How much does it cost?
- It's all free and open source blah blah
- Nikos buzzwords


* Fake testimonials

** 1
I love it!  Now we have this whole stack set up, I can finally get rid
of my regular expressions manual!

** 2
The days of keeping gigabytes of gzipped log files are over.  This
truly is a new dawn in analysing usage data.

** 3
At first I was hesitant to let go of my old Apache log format, but
Kibana makes it so easy to see patterns in my data that I've really
seen the light.
